deploy: True
tag: scratch
seed: 42

device_id: 'cuda:0'

data:
  epochs: 1
  num_total_steps: 200_000 # Num_total_tokens = num_total_steps * batch_size * context_length
  context_length: 500
  batch_size: 100 # We could optimize batch size for faster convergence.
  dtype: "bfloat16"
  hf_name: "monology/pile-uncopyrighted" # could do HuggingFaceFW/fineweb as well
  num_workers: 2
  cache_dir: "../../activations/precomputed_activations/precomputed_activations_llama3/"

llm:
  model_hf_name: "meta-llama/Llama-3.1-8B"
  tokenizer_hf_name: "meta-llama/Llama-3.1-8B"
  dimin: 4096

sae:
  sae_type: 'temporal'
  sae_diff_type: 'topk' # Options: 'relu', 'topk', 'nullify'
  tied_weights: True
  block_id: 1
  exp_factor: 4 # can do 4 if 8 is too big to train
  kval_topk: 256 # If possible, do 128 and 512 as well.
  n_heads: 4 # Maybe an additional one wiht more heads and lower bottleneck_factor? This is low priority, bigger expansion factor is more important. Does it make sense to keep (n_heads * bottleneck_factor = d_model) as done in many transformer implementations?
  n_attn_layers: 1
  bottleneck_factor: 1 # Could put this lower and increase the number of heads?
  gamma_reg: 10 # Not sure
  scaling_factor: 0.085
  
optimizer: # I don't have a good intuition fo the optimizer parameters.
  learning_rate: 1e-3
  weight_decay: 1e-4
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  decay_lr: True
  warmup_iters: 200
  min_lr: 9e-4

eval:
  save_tables: False

log: 
  save_multiple: False
  log_interval: 10
  save_interval: 1000
  wandb_project_name: "TemporalSAE_test"


# Nested configs. Disable hydra logging
defaults:
  - _self_
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled

# Disable hydra directory structure
hydra:
  output_subdir: Null
  job:
    chdir: False
  run:
    dir: .

  sweep:
    dir: .
    subdir: .
